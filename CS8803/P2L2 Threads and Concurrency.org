* 01 - LESSON PREVIEW
  THREADS AND CONCURRENCY
** What are threads?
** How are threads different from processes?
** What data structures are used to implement and manage threads?
** "An Introduction to Programming with Threads" by Birrell
   - Threads and concurrency
   - Basic mechanisms for multithreaded systems
   - Synchronization
* 02 - VISUAL METAPHOR
  A THREAD is like a WORKER IN A TOY SHOP
** Worker in a Toy Shop
   - is an active entity
     - executing a unit of toy order
   - works simultaneously with others
     - many workers completing toy orders
   - requires coordination
     - sharing of tools, parts, workstations, ...
** Thread
   - is an active entity
     - executing unit of a process
   - works simultaneously with others
     - many threads executing (concurrency)
   - requires coordination
     - sharing of I/O devices, CPUs, memory... (underlying system resources)
* 03 - PROCESS VS THREAD
  A single-threaded process is represented by its address space (PCB)
** Single-Threaded Process
   - code, data, files
   - data registers and stack
** Multi-Threaded Process
   - code, data, and files are shared
   - registers and stack are per thread
   - the PCB holds data that is common (virtual address mapping, description about
     code and data) across threads as well as an execution context per thread which
     holds info like stack pointer, program counter, etc.
* 04 - BENEFITS OF MULTITHREADING
** Parallelization => speed-up
   Each thread (up to a point) gets a dedicated CPU
   - different threads for different portions of the input matrix
   - different threads can be dedicated to different I/O tasks such as input 
     processing or display rendering
   - different threads to executed different functions of the code
** Specialization => hot cache!
   Each thread has access to its own cache, so specialization can minimize swapping
   out cache data.
   - can prioritize threads based on their specialized task
   - if instead of multiple threads, we wrote a multi-process application, we would
     now require individual address space (higher memory requirements) as well as
     individual execution context per process (IPC considerations?)
** Efficiency => lower resource requirements
   - lower memory (no separate address space overhead)
   - cheaper shared data/communication than IPC
* 05 - ARE THREADS USEFUL ON A SINGLE CPU?
  Or when #threads > #CPUs
  If a thread (T1) is waiting for I/O from disk, e.g., the thread is idle (t_idle).
  If t_idle is long enough, it might make sense to context switch to another thread
  (T2), process some instructions, and context switch back. This will hide some
  program latency. The main overhead associated with context switching comes from
  creating a new virtual-to-physical memory mapping. Since threads share address
  space, this isn't an issue like it is with swapping processes.
** if t_idle > 2*t_ctx_switch, then context switch to hide idling time
** t_ctx_switch threads < t_ctx_switch processes
* 06 - BENEFITS OF MULTITHREADING APPS AND OS
** Multithreaded OS Kernel
   - threads working on behalf of applications
   - OS-level services like daemons or drivers
* 07 - PROCESS VS THREADS QUIZ
  Do the following statements apply to processes (P), threads (T), or both (B)?
  [T] can share a virtual address space
  [P] take longer to context switch
  [B] have an execution context
  [T] usually result in hotter caches when multiple exist
  [B] make use of some communication mechanisms
* 09 - BASIC THREAD MECHANISMS
  What do we need to support threads?
** Thread Data Structure
   - identify threads, keep track of resource usage...
** Mechanisms to Create and Manage Threads
** Mechanisms to Safely Coordinate Among Threads Running Concurrently in the Same
   Address Space
   - ensure threads don't overwrite each other's input or results
   - ensure one thread can wait on another if it depends on the other's result
*** Data encapsulation in processes
    - if there is a virtual address from P1 which can access physical address x,
      then there is no virtual address in process P2 which has access to that same
      physical mapping to x
*** Data sharing in threads
    - since threads share the same virtual address space, they share the mapping to 
      physical storage as well, so each thread has access to the same data
    - this can result in data races where two threads are updating the same data or
      one is attempting to read data that the other thread is writing, etc.
** Concurrency Control & Coordination
   - Mutual Exclusion
     - exclusive access to only one thread at a time
     - mutex
   - Waiting on Other Threads
     - specific condition before proceeding
     - condition variable
     - if a thread is waiting for other threads to process an order before it can 
       handle shipping of the order, it doesn't make sense to keep polling the state
       of the order. Instead, the thread should be notified on completion of the order
       so it can then pick up the completed order and do its task to ship it.
   - Waking up Other Threads from Wait State
* 10 - THREADS AND THREAD CREATION
** Thread Type
   - thread data structure with: thread ID, PC, SP, registers, stack, attributes
** Fork(proc, args)
   fork call with two params: procedure that created thread will start executing,
   and arguments for the procedure
   - create a thread
   - not UNIX fork (for forking processes)
*** in T0, t1 = fork(proc, args) => T1
    The data structure for the newly created thread T1 is:
    - PC = proc (program counter points to the first instruction of the procedure)
    - stack = args (arguments will be available on the stack)
*** what happens when T1 ends, either with return result, or exit (success or error)
    - place result in well-defined place in shared address space
    - notification mechanism for parent or other threads
** Join(thread)
   When the parent thread calls join with the thread id of the child thread, it will
   be blocked until the child completes. Join will return to the parent the result
   of the child thread's computation. At that point any resources (memory, data)
   solely belonging to the child are released back to the system and the child
   thread will be terminated.
   - terminate a thread
   - child.result = join(t1)
   - aside from join, where the parent is joining the child, in all other aspects,
     the two threads are completely equivalent, having access to the same data, etc.
* 11 - THREAD CREATION EXAMPLE
  Two threads (the main thread of the process and the one created by fork)
  #+BEGIN_SRC c
    Thread thread1;
    Shared_list list;
    thread1 = fork(safe_insert, 4);
    safe_insert(6);
    join(thread1); // Optional
  #+END_SRC
  T0 - parent thread
  thread1 = fork(safe_insert, 4) -> T1
  Depending on whether T0 executes its safe_insert(6) before or after child T1
  executes its safe_insert(4), the list could look like 4->6->NULL or 6->4->NULL.
* 12 - MUTEXES
** How is the List Updated?
  - each list element has two fields: a value field and a pointer to the next element
  - list points to the head of the list
  - items are placed at the head of the list
  - take what head is pointing to and set that as the next pointer of the new element
  - now point head to the new element
** In Pseudocode
   create new list element e
   set e.value = X
   read list and list.p_next
   set e.p_next = list.p_next
   set list.p_next = e
** Conflict
   If two threads running on two CPUs simultaneously try to update the list, we could
   end up with a situation where we have an orphan element because if both threads
   read the value of the head pointer before either of them update it, the first
   new element will point to the original head element only until the second thread
   inserts its element, at which point it too will point to the original head
   element and the other new element will be skipped over and lost forever because
   there is no list element to point to it.
* 13 - MUTUAL EXCLUSION
** Mutex
   - like a lock that should be used whenever there is shared access to data or state
   - as data structure: locked?, owner, blocked_threads
   - Lock(mutex) { "Critical Section" }
   - any operation that can only be done by one thread at a time should be placed
     in the critical section (within a lock operation). Things like updating a 
     shared variable or incrementing/decrementing a counter
   - lock is implicitly freed at the close of the curly braces (Burrell)
   - most common APIs have an explicit Lock(m) and Unlock(m)
* 14 - MUTEX EXAMPLE
  Making safe_insert safe
  #+BEGIN_SRC c
    list<int> my_list;
    Mutex m;
    void safe_insert(int i)
    {
            Lock(m) {
                    my_list.insert(i);
            } // unlock
    }
  #+END_SRC
  - T0 forks, creating T1
  - T0 calls safe_insert(6), acquires lock
  - T1 tries safe_insert(4) but is blocked
  - T0 inserts 6 into the list: 6->NULL
  - T0 releases the lock
  - T1 acquires the lock
  - T1 inserts 4 into the list: 4->6->NULL
  - T1 releases the lock
* 15 - MUTEX QUIZ
  Threads T1-T5 are contending for a mutex m. T1 is the first to obtain the mutex.
  Which thread will get access to m after T1 releases it? Mark all that apply.
** TIME -------->
  T1-----lock(m)========|     *pipe identifies time T1 releases lock
  T2-----------lock(m)  |
  T3--------------------|---lock(m)
  T4-------lock(m)      |
  T5--------------------|lock(m)

  [X] T2
  [-] T3
  [X] T4
  [X] T5

  While T4's request for the lock came before T2, both are placed in a pending queue.
  However, in the mutex queue there is no guarantee of order, so both are at equal
  opportunity to obtain the lock next. T3 makes its request some time after T1
  releases the lock. Since there are already pending requests, those requests will
  be available to take the lock at the time of release and T3 doesn't stand a chance.
  The situation with T5 is trickier. T5 is requesting the lock from a separate CPU at
  the exact time that the lock is being released. If T5 makes the request before the
  operation that looks for the next candidate for the lock, it could very well be in
  the running to get the lock next.
* 17 - PRODUCER / CONSUMER EXAMPLE
  What if the processing you wish to perform with mutual exclusion needs to occur
  only under certain conditions?
  Consider the case where there are producer threads placing values into a shared list.
  There is also one special consumer thread that is waiting for the list to become
  full so it can read/process the data and clear the list.
  #+BEGIN_SRC c
    // Producer / Consumer Pseudocode
    // main
    for i=0..10
            producers[i] = fork(safe_insert, NULL) // create producers
    consumer = fork(print_and_clear, my_list)      // create consumer

    // producers: safe_insert
    Lock(m) {
            list->insert(my_thread_id)
    } // unlock

    // consumer: print_and_clear
    Lock(m) {
            if my_list.full -> print; clear_to_limit
            else -> release lock and try again
    } // unlock
  #+END_SRC
  Constantly locking and trying and releasing is WASTEFUL!!
* 18 - CONDITION VARIABLES
** Consumer With Wait
   During Wait, Consumer suspends itself. Mutex must be automatically released
   when going into the WAIT state and automatically re-acquired when coming out of
   the WAIT state.
   #+BEGIN_SRC c
     // consumer: print_and_clear
     Lock(m) {
             while (my_list.not_full()) {
                     Wait(m, list_full);
             }
             my_list.print_and_remove_all();
     } // unlock
   #+END_SRC
** Producer With Signal
   #+BEGIN_SRC c
     // producers: safe_insert
     Lock(m) {
             my_list.insert(my_thread_id);

             if (my_list.full()) {
                     Signal(list_full);
             }
     } // unlock
   #+END_SRC
* 19 - CONDITION VARIABLE API
  To summarize, a common condition variable API will look as follows:
** Condition type
** Wait(mutex, cond)
   - mutex is automatically released and re-acquired on wait
   #+BEGIN_SRC c
     Wait (mutex, cond) {
             // atomically release mutex
             // and go on wait queue

             // ... wait ... wait ... wait ...

             // remove from queue
             // re-acquire mutex
             // exit the wait operation
     }
   #+END_SRC
** Signal(cond)
   - notify only one thread waiting on condition
** Broadcast(cond)
   - notify all waiting threads
** Condition Variable Data Structure
   - mutex reference
   - waiting threads
   - ...
* 20 - CONDITION VARIABLE QUIZ
  Recall the consumer code from the previous example for condition variables
  #+BEGIN_SRC c
    // consumer: print_and_clear
    Lock(m) {
            while (my_list.not_full()) {
                    Wait(m, list_full);
            }
            my_list.print_and_remove_all();
    } // unlock
  #+END_SRC
  Instead of "while", why did we not simply use "if"?
  [-] "while" can support multiple consumer threads
  [-] cannot guarantee access to m once the condition is signaled
  [-] the list can change before the consumer gets access again
  [X] all of the above
* 22 - READERS / WRITER PROBLEM
  There is some shared resource that can be read by multiple readers, but only
  written to by one writer at a time. If the resource is being written to, it may
  not be read.
  The naive approach is to put a mutex around access to the resource, but this is
  too restrictive to readers since a mutex would only allow one thread at a time
  and we want to allow multiple readers.
** Readers
   0 or more
** Writer
   0 or 1
** Summary
   if ((read_counter == 0) && (write_counter == 0))
   - R_ok, W_ok
   if (read_counter > 0)
   - R_ok
   if (write_counter == 1)
   - R_no, W_no
** Represented by State
   State of shared file/resource
   - FREE: resource_counter = 0
   - READING: resource_counter > 0
   - WRITING: resource_counter = -1
   Now that we have a state variable, using this one level of redirection, we can
   place a mutex around this variable to control access to updates on this variable
   which will mark what actions are allowable for the resource.
* 23 - READER / WRITER EXAMPLE, PART 1
  #+BEGIN_SRC c
    Mutex counter_mutex;
    Condition read_phase, write_phase;
    int resource_counter = 0;

    // READERS
    Lock(counter_mutex)
    {
            while (resource_counter == -1) {
                    Wait(counter_mutex, read_phase);
            }
            resource_counter++;
    } // unlock
    // ... read data ...
    Lock(counter_mutex)
    {
            resource_counter--;
            if (resource_counter == 0) {
                    Signal(write_phase);
            }
    } // unlock;

    // WRITER
    Lock(counter_mutex)
    {
            while (resource_counter != 0) {
                    Wait(counter_mutex, write_phase);
            }
            resource_counter = -1;
    } // unlock
    // ... write data ...
    Lock(counter_mutex)
    {
            resource_counter = 0;
            Broadcast(read_phase);
            Signal(write_phase);
    } // unlock;
  #+END_SRC
* 26 - CRITICAL SECTION STRUCTURE
** Enter Critical Section Code
   ~LOCK
** Read or Write Section
** Exit Critical Section Code
   ~UNLOCK

** Typical Critical Section Structure
   #+BEGIN_SRC c
     Lock(mutex)
     {
             while (!predicate_indicating_access_ok) {
                     wait(mutex, cond_var);
             }

             update state => update predicate;

             signal and/or broadcast (cond_var_with_correct_waiting_threads)
     } // unlock;
   #+END_SRC
* 27 - CRITICAL SECTION STRUCTURE WITH PROXY
  #+BEGIN_SRC c
    // ENTER CRITICAL SECTION
    perform_critical_operation(read_write_shared_file)
    // EXIT CRITICAL SECTION

    // ENTER CRITICAL SECTION
    Lock(mutex)
    {
            while (!predicate_for_access) {
                    wait(mutex, cond_var);
            }
            update_predicate;
    } // unlock;

    // EXIT CRITICAL SECTION
    Lock(mutex)
    {
            update_predicate;
            signal_broadcast(cond_var);
    } // unlock;
  #+END_SRC
* 28 - AVOIDING COMMON MISTAKES
** Keep track of mutex/condition variables used with a resource
   - e.g., mutex_type m1; // mutex for file1 (important contextual comment)
** Check that you are always (and correctly) using lock and unlock
   - e.g., did you forget to lock/unlock? what about compilers (helpful warnings)?
** Use a single mutex to access a single resource!
** Check that you are signaling the correct condition
** Check that you are not using signal when broadcast is needed
   - signal: only 1 thread will proceed... remaining threads will continue to wait...
     possibly indefinitely!!!
** Ask yourself: do you need priority guarantees?
   - thread execution order not controlled by signals to condition variables!
** Spurious Wake-ups
** Deadlocks
* 29 - SPURIOUS WAKE-UPS
  Spurious wake-ups == when we wake threads knowing they may not be able to proceed.
  - pitfall that doesn't affect correctness but may impact performance.
  #+BEGIN_SRC c
    // WRITER
    Lock(counter_mutex)
    {
            resource_counter = 0;
            Broadcast(read_phase);
            Signal(write_phase);
    } // unlock;

    // READERS
    // elsewhere in the code...
    Wait(counter_mutex, write/read_phase);
  #+END_SRC
  In the above situation, we see a Broadcast call from the writer. This wakes up any
  readers who are waiting. They will attempt to get the mutex, but it may still be
  locked because the writer has yet to run the Signal line and still has the lock.
  These readers will no longer be in the Wait queue because they have been woken up
  by the broadcast of the updated condition variable, but they must now wait in the
  counter_mutex queue because they are unable to acquire the lock. This is what is
  known as a spurious wake-up.

  The way to avoid this in the case of the Writer is to change the code to the
  following:
  #+BEGIN_SRC c
    // WRITER
    Lock(counter_mutex)
    {
            resource_counter = 0;
    } // unlock;

    Broadcast(read_phase);
    Signal(write_phase);
  #+END_SRC
  So, by placing the Broadcast and Signal outside of the lock, we know that when
  another thread is woken up, we have already released the lock. In the case of the
  following Readers, however, we cannot place the Signal outside the lock because
  it relies on an if check on the very condition variable that we are using the lock
  for.
  #+BEGIN_SRC c
    // IN READERS
    Lock(counter_mutex)
    {
            resource_counter--;
            if (resource_counter == 0) {
                    Signal(write_phase);
            }
    } // unlock;
  #+END_SRC
* 30 - DEADLOCKS INTRODUCTION
** Definition
   Two or more competing threads are waiting on each other to complete, but none of
   them ever does.
** Toy Shop Example
   In order for a worker to complete their task, they need to solder some component.
   They need access to both the soldering iron and solder. One worker has the
   soldering iron and is waiting on the solder. The other has the solder but is
   waiting on the soldering iron. Neither will give up what they have so they are
   deadlocked.
* 31 - DEADLOCKS
** Scenario
   T1 and T2 both need to acquire locks for shared variables A and B because T1
   needs to execute foo1(A,B) and T2 needs to execute foo2(A,B). If the order of
   acquisition for T1 is lock(m_A) then lock(m_B) and the order of acquisition for
   T2 is lock(m_B) then lock(m_A), then when T1 attempts to acquire the lock for B,
   it is already held by T2, so it waits. Likewise, when T2 attempts to acquire the
   lock for A, it is already held by T1, so it waits. This is the exact condition
   for a deadlock because each thread is waiting on the other in a cycle.
** How to Avoid This?
*** Unlock A before locking B => fine-grained locking
    - CON: threads need both A & B for the same operation
*** Get all locks up front, then release at the end
    OR use one MEGA lock
    - PRO: for some applications, ok!
    - CON: too restrictive => limits parallelism
*** Maintain lock order
    First m_A, then m_B
    - PRO: will prevent cycles in wait graph; foolproof
    - CON: may take some effort to ensure that order is adhered to in all instances
* 32 - DEADLOCKS SUMMARY
** Summary
   A cycle in the wait graph is NECESSARY and SUFFICIENT for a deadlock to occur. The
   wait graph is a graph where the edges go from a thread waiting on a resource to a
   thread owning a resource.
** What can we do about it?
*** Deadlock prevention EXPENSIVE
    Will a proposed operation result in a cycle in the graph?
    If so, we might need to release a resource before we perform the lock request
*** Deadlock detection & recovery ROLLBACK
    Analyze the graph, maintain the ability to roll back by maintaining enough state
*** Apply the Ostrich Algorithm REBOOT
    Do nothing, and if all else fails, reboot
** Summary of options
   Deadlocks are a reality, but some of the solutions to deal with them are expensive
   or difficult to implement if there are mutexes coming from different source files,
   etc. There is overhead associated with both of the above solutions to deadlocks,
   so for that reason, these techniques are usually only applied in performance-
   critical applications.
* 33 - CRITICAL SECTION QUIZ
  A toy shop has the following policy:
  At any point in time:
  - max 3 new orders can be processed
  - if only 1 new order is being processed, then any number of old orders can be 
    processed
    
  #+BEGIN_SRC c
    // toy_shop_entry_for_new_orders
    lock(orders_mutex)
    {
            /** [INSERT CHECK HERE] **/ {
                    wait(orders_mutex, new_cond);
            }
            new_order++;
    }
  #+END_SRC

  Select the appropriate check that needs to be made for the critical section.
  Check all that apply.

  [-] while ((new_order == 3) || (new_order == 1 && old_order > 0))
  [-] if ((new_order == 3) || (new_order == 1 && old_order > 0))
  [-] while ((new_order >= 3) || (new_order == 1 && old_order >= 0))
  [-] while ((new_order >= 3) || (new_order == 1 && old_order >= 1))
